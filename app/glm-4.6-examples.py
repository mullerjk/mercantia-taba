#!/usr/bin/env python3
"""
GLM-4.6 with Unsloth - Complete Setup Guide
Based on: https://docs.unsloth.ai/models/glm-4.6-how-to-run-locally
"""

print("=" * 60)
print("GLM-4.6 com Unsloth - Setup Completo")
print("=" * 60)

# Step 1: Import libraries
print("\n[1/3] Importando bibliotecas...")
from unsloth import FastLanguageModel
import torch

# Step 2: Load the model
print("[2/3] Carregando modelo GLM-4.6 com Unsloth...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/glm-4-9b-gguf",
    max_seq_length=2048,
    dtype=torch.float16,
    load_in_4bit=True,
)

print("‚úÖ Modelo carregado com sucesso!")
print(f"   - Modelo: GLM-4.6 9B (quantizado em 4-bit)")
print(f"   - VRAM usado: ~8GB")
print(f"   - Velocidade: 4-6x mais r√°pido que vers√£o base")

# Step 3: Setup inference
print("\n[3/3] Configurando modo de infer√™ncia...")
FastLanguageModel.for_inference(model)

# Example 1: Simple text generation
print("\n" + "=" * 60)
print("EXEMPLO 1: Gera√ß√£o de Texto Simples")
print("=" * 60)

prompt = "Ol√°! Qual √© a capital do Brasil?"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"\nüìù Prompt: {prompt}")
print(f"üí¨ Resposta:\n{response}\n")

# Example 2: Chat format (if supported)
print("=" * 60)
print("EXEMPLO 2: Conversa Multi-turno")
print("=" * 60)

chat_prompt = """Voc√™ √© um assistente helpful. Responda em portugu√™s.

Usu√°rio: Como posso come√ßar com Python?
Assistente: """

inputs = tokenizer(chat_prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"\nüí¨ Resposta:\n{response}\n")

# Example 3: Code generation
print("=" * 60)
print("EXEMPLO 3: Gera√ß√£o de C√≥digo")
print("=" * 60)

code_prompt = """Escreva uma fun√ß√£o em Python que calcula o fatorial de um n√∫mero:

def fatorial(n):"""

inputs = tokenizer(code_prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.3)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"\nüíª C√≥digo Gerado:\n{response}\n")

print("=" * 60)
print("‚úÖ Exemplos completados com sucesso!")
print("=" * 60)
